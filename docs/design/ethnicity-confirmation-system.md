# Ethnicity Confirmation & Record Tracing System Design

## ðŸŽ¯ **Executive Summary**

Design for complete ethnicity confirmation lifecycle management with precise record tracing. Enables seamless workflow from AI prediction â†’ dialler team confirmation â†’ learning system improvement. Core requirement to write enriched files with AI ethnicity + empty confirmation columns for dialler team workflow.

## ðŸ“‹ **Business Requirements**

### **Core Workflow:**
1. **AI Processing**: System classifies director ethnicities from names with confidence scores
2. **Export Enhancement**: Original data + AI ethnicity + empty confirmation dropdown for manual entry
3. **Dialler Team**: Makes calls, confirms ethnicities during right-party contact and interest expression
4. **Confirmation Upload**: Validated ethnicity confirmations fed back to improve system accuracy
5. **Learning Integration**: Confirmed data enhances spatial and name-based pattern learning

### **Key Requirements:**
- **Exact Record Tracing**: Know which original row each ethnicity came from
- **Spatial Context**: Track name+suburb+city combinations for correlation analysis
- **Canonical Validation**: Only accept valid ethnicity values from predefined list
- **Excel Integration**: Dropdown validation with common ethnicities listed first
- **Learning Feedback**: Confirmed ethnicities improve AI accuracy over time

## ðŸ“Š **Enhanced Database Schema**

### **Core Ethnicity Lifecycle Tables**

```sql
-- Enhanced lead processing results with source tracking
ALTER TABLE lead_processing_results ADD COLUMN source_row_number INTEGER NOT NULL;
ALTER TABLE lead_processing_results ADD COLUMN source_file_identifier TEXT NOT NULL;
ALTER TABLE lead_processing_results ADD COLUMN original_entity_name TEXT;
ALTER TABLE lead_processing_results ADD COLUMN original_director_name TEXT;
ALTER TABLE lead_processing_results ADD COLUMN original_registered_address TEXT;
ALTER TABLE lead_processing_results ADD COLUMN original_registered_city TEXT;
ALTER TABLE lead_processing_results ADD COLUMN original_registered_province TEXT;

-- Ethnicity confirmation lifecycle management
CREATE TABLE ethnicity_confirmations (
    confirmation_id TEXT PRIMARY KEY,
    
    -- Source tracing (exact record identification)
    source_file_identifier TEXT NOT NULL,  -- filename + hash for uniqueness
    source_row_number INTEGER NOT NULL,    -- 1-based Excel row number
    source_job_id TEXT NOT NULL,           -- Processing job reference
    
    -- Original data preservation
    original_entity_name TEXT NOT NULL,
    original_director_name TEXT NOT NULL,
    original_registered_address TEXT,
    original_registered_city TEXT,
    original_registered_province TEXT,
    
    -- Spatial context for correlation analysis
    canonical_suburb TEXT,                 -- Cleaned suburb name
    canonical_city TEXT,                   -- Cleaned city name  
    canonical_province TEXT,               -- Cleaned province name
    spatial_context_hash TEXT,             -- Hash for quick spatial lookups
    
    -- AI prediction data
    ai_predicted_ethnicity TEXT NOT NULL,
    ai_confidence_score REAL NOT NULL,
    ai_classification_method TEXT NOT NULL, -- rule_based|phonetic|llm|cache
    
    -- Human confirmation data
    confirmed_ethnicity TEXT,              -- Must match canonical_ethnicities
    confirmed_by TEXT,                     -- Sales rep identifier
    confirmed_at TIMESTAMP,               -- When confirmation occurred
    confirmation_notes TEXT,               -- Optional notes from call
    confirmation_source TEXT,              -- phone_call|meeting|email|other
    
    -- Metadata
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    UNIQUE(source_file_identifier, source_row_number),
    FOREIGN KEY (source_job_id) REFERENCES job_executions(job_id),
    FOREIGN KEY (confirmed_ethnicity) REFERENCES canonical_ethnicities(ethnicity_code)
);

-- Canonical ethnicity definitions with strict validation
CREATE TABLE canonical_ethnicities (
    ethnicity_code TEXT PRIMARY KEY,       -- System identifier
    ethnicity_display_name TEXT NOT NULL, -- Human-readable name
    ethnicity_order INTEGER NOT NULL,     -- Order for dropdowns (common first)
    ethnicity_description TEXT,           -- Optional description
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Initial canonical ethnicity data (SA-specific)
INSERT INTO canonical_ethnicities (ethnicity_code, ethnicity_display_name, ethnicity_order) VALUES
('african', 'African', 1),                    -- Most common first
('white', 'White', 2),
('coloured', 'Coloured', 3),
('indian', 'Indian', 4),
('asian', 'Asian', 5),
('cape_malay', 'Cape Malay', 6),
('other', 'Other', 7),
('unknown', 'Unknown', 8),
('declined', 'Declined to State', 9);

-- Enhanced spatial ethnicity patterns with confirmation tracking
CREATE TABLE spatial_ethnicity_patterns (
    pattern_id TEXT PRIMARY KEY,
    
    -- Pattern definition
    name_component TEXT NOT NULL,          -- First name, surname, or component
    suburb TEXT,                          -- Spatial context (most predictive)
    city TEXT,
    province TEXT,
    
    -- Pattern effectiveness
    ethnicity_code TEXT NOT NULL,         -- Predicted ethnicity
    confidence_score REAL NOT NULL,       -- Pattern confidence
    confirmation_count INTEGER DEFAULT 0,  -- Human confirmations
    total_applications INTEGER DEFAULT 0,  -- Total times pattern applied
    success_rate REAL DEFAULT 0.0,        -- confirmation_count / total_applications
    
    -- Pattern lifecycle
    created_from_job_id TEXT,             -- Job that created pattern
    last_confirmed_at TIMESTAMP,
    last_applied_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (ethnicity_code) REFERENCES canonical_ethnicities(ethnicity_code),
    FOREIGN KEY (created_from_job_id) REFERENCES job_executions(job_id)
);

-- File processing tracking for precise record management
CREATE TABLE file_processing_sessions (
    session_id TEXT PRIMARY KEY,
    source_file_path TEXT NOT NULL,
    source_file_identifier TEXT NOT NULL,  -- filename + content hash
    source_file_size INTEGER,
    source_file_modified_time INTEGER,     -- File mtime
    total_rows INTEGER,
    processed_rows INTEGER DEFAULT 0,
    
    -- Processing metadata
    job_id TEXT NOT NULL,
    processing_started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_completed_at TIMESTAMP,
    processing_status TEXT DEFAULT 'running', -- running|completed|failed
    
    FOREIGN KEY (job_id) REFERENCES job_executions(job_id)
);

-- Indexes for performance
CREATE INDEX idx_ethnicity_confirmations_source ON ethnicity_confirmations(source_file_identifier, source_row_number);
CREATE INDEX idx_ethnicity_confirmations_spatial ON ethnicity_confirmations(canonical_suburb, canonical_city, original_director_name);
CREATE INDEX idx_spatial_patterns_lookup ON spatial_ethnicity_patterns(name_component, suburb, city);
CREATE INDEX idx_spatial_patterns_effectiveness ON spatial_ethnicity_patterns(success_rate DESC, confirmation_count DESC);
```

### **File Identification System**

```python
def generate_file_identifier(file_path):
    """Generate unique identifier for source file tracking."""
    file_name = Path(file_path).name
    
    # Hash file content for uniqueness (MD5 sufficient for identification)
    with open(file_path, 'rb') as f:
        content_hash = hashlib.md5(f.read()).hexdigest()[:8]
    
    # Format: filename_hash (e.g., "leads_2024_a1b2c3d4")
    return f"{file_name.split('.')[0]}_{content_hash}"

def generate_spatial_context_hash(director_name, suburb, city, province):
    """Generate hash for spatial context lookups."""
    components = [
        normalize_name(director_name),
        normalize_place(suburb) if suburb else '',
        normalize_place(city) if city else '',
        normalize_place(province) if province else ''
    ]
    context_string = '|'.join(filter(None, components))
    return hashlib.sha256(context_string.encode()).hexdigest()[:16]
```

## ðŸ“ **Enhanced Export Schema**

### **Excel Output Format**

```
Original Lead Columns (11):
1.  EntityName
2.  TradingAsName
3.  Keyword
4.  ContactNumber
5.  CellNumber
6.  EmailAddress
7.  RegisteredAddress
8.  RegisteredAddressCity
9.  RegisteredAddressProvince
10. DirectorName
11. DirectorCell

AI Enhancement Columns (5):
12. director_ethnicity              # AI Classification (African, White, etc.)
13. ethnicity_confidence           # Confidence score (0.95, 0.73, etc.)
14. classification_method          # Method used (rule_based, llm, phonetic, cache)
15. spatial_context                # Suburb/City for context
16. processing_notes               # Any processing warnings or notes

Confirmation Columns (2):
17. confirmed_ethnicity            # EMPTY - Dropdown validation for manual entry
18. confirmation_notes             # EMPTY - For dialler team notes

Metadata Columns (3):
19. source_row_number              # Original Excel row (for tracing)
20. job_id                         # Processing job reference
21. processed_at                   # Processing timestamp
```

### **Excel Dropdown Implementation**

```python
def create_ethnicity_dropdown_validation(worksheet, start_row, end_row, column_letter='Q'):
    """Create dropdown validation for confirmed_ethnicity column."""
    
    # Get ethnicity options in order of frequency/importance
    ethnicity_options = get_canonical_ethnicities_for_dropdown()
    
    from openpyxl.worksheet.datavalidation import DataValidation
    
    validation = DataValidation(
        type="list",
        formula1=f'"{",".join(ethnicity_options)}"',
        showDropDown=True,
        showErrorMessage=True,
        errorTitle="Invalid Ethnicity Selection",
        error="Please select a valid ethnicity from the dropdown list. Contact admin to add new options.",
        showInputMessage=True,
        promptTitle="Select Ethnicity",
        prompt="Choose the confirmed ethnicity from the dropdown. Most common options are listed first."
    )
    
    # Apply validation to confirmed_ethnicity column
    validation.add(f"{column_letter}{start_row}:{column_letter}{end_row}")
    worksheet.add_data_validation(validation)

def get_canonical_ethnicities_for_dropdown():
    """Get ordered list of ethnicities for dropdown."""
    query = """
    SELECT ethnicity_display_name 
    FROM canonical_ethnicities 
    WHERE is_active = true 
    ORDER BY ethnicity_order
    """
    return [row[0] for row in execute_query(query)]
```

### **Confidence-Based Color Coding**

```python
def apply_confidence_color_coding(worksheet, start_row, end_row):
    """Apply color coding based on AI confidence levels."""
    from openpyxl.styles import PatternFill
    
    confidence_colors = {
        'high': 'C6EFCE',      # Light green (>0.8)
        'medium': 'FFEB9C',    # Light yellow (0.6-0.8)
        'low': 'FFC7CE',       # Light red (<0.6)
        'very_low': 'E6E6FA'   # Light purple (<0.4)
    }
    
    for row_num in range(start_row, end_row + 1):
        # Get confidence value from column M (ethnicity_confidence)
        confidence_cell = worksheet[f'M{row_num}']
        ethnicity_cell = worksheet[f'L{row_num}']  # director_ethnicity
        
        if confidence_cell.value:
            confidence = float(confidence_cell.value)
            
            if confidence >= 0.8:
                color = confidence_colors['high']
            elif confidence >= 0.6:
                color = confidence_colors['medium']
            elif confidence >= 0.4:
                color = confidence_colors['low']
            else:
                color = confidence_colors['very_low']
            
            # Apply color to ethnicity cell
            ethnicity_cell.fill = PatternFill(
                start_color=color, 
                end_color=color, 
                fill_type='solid'
            )
```

## ðŸ”„ **Complete Workflow Implementation**

### **Enhanced Job Processing with Source Tracking**

```python
class EnhancedJobProcessor:
    """Enhanced job processor with ethnicity confirmation support."""
    
    def __init__(self, input_file_path):
        self.input_file_path = Path(input_file_path)
        self.file_identifier = generate_file_identifier(input_file_path)
        self.ethnicity_classifier = EthnicityClassifier()
        self.confirmation_db = EthnicityConfirmationDatabase()
    
    async def process_leads_with_confirmation_support(self, job_id):
        """Process leads with full confirmation lifecycle support."""
        
        # Read Excel file
        df = pd.read_excel(self.input_file_path)
        
        # Create file processing session
        session_id = await self._create_processing_session(job_id, len(df))
        
        results = []
        
        for row_index, row_data in df.iterrows():
            # Extract data
            entity_name = row_data['EntityName']
            director_name = row_data['DirectorName']
            registered_address = row_data['RegisteredAddress']
            registered_city = row_data['RegisteredAddressCity']
            registered_province = row_data['RegisteredAddressProvince']
            
            # Classify ethnicity
            classification = await self.ethnicity_classifier.classify(
                name=director_name,
                city=registered_city,
                province=registered_province
            )
            
            # Generate spatial context
            spatial_hash = generate_spatial_context_hash(
                director_name, None, registered_city, registered_province
            )
            
            # Create confirmation record
            confirmation_record = {
                'confirmation_id': generate_uuid(),
                'source_file_identifier': self.file_identifier,
                'source_row_number': row_index + 2,  # Excel 1-based + header row
                'source_job_id': job_id,
                
                # Original data
                'original_entity_name': entity_name,
                'original_director_name': director_name,
                'original_registered_address': registered_address,
                'original_registered_city': registered_city,
                'original_registered_province': registered_province,
                
                # Spatial context
                'canonical_suburb': None,  # Will be filled by address cleaning
                'canonical_city': registered_city,
                'canonical_province': registered_province,
                'spatial_context_hash': spatial_hash,
                
                # AI prediction
                'ai_predicted_ethnicity': classification['ethnicity'],
                'ai_confidence_score': classification['confidence'],
                'ai_classification_method': classification['method'],
                
                # Confirmation fields (empty initially)
                'confirmed_ethnicity': None,
                'confirmed_by': None,
                'confirmed_at': None,
                'confirmation_notes': None,
            }
            
            # Store confirmation record
            await self.confirmation_db.store_confirmation_record(confirmation_record)
            
            # Prepare result for export
            export_record = {
                **row_data.to_dict(),  # Original columns
                
                # AI enhancement columns
                'director_ethnicity': classification['ethnicity'],
                'ethnicity_confidence': classification['confidence'],
                'classification_method': classification['method'],
                'spatial_context': f"{registered_city}, {registered_province}",
                'processing_notes': classification.get('notes', ''),
                
                # Empty confirmation columns
                'confirmed_ethnicity': '',
                'confirmation_notes': '',
                
                # Metadata columns
                'source_row_number': row_index + 2,
                'job_id': job_id,
                'processed_at': datetime.now().isoformat(),
            }
            
            results.append(export_record)
        
        await self._complete_processing_session(session_id)
        return results
```

### **Enhanced Export with Confirmation Support**

```python
def export_with_confirmation_columns(job_id, output_path=None, format='excel'):
    """Export job results with ethnicity confirmation columns."""
    
    # Generate output path if not provided
    if not output_path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"enriched_leads_{job_id[:8]}_{timestamp}.xlsx"
    
    # Get job results with confirmation data
    results = get_job_results_with_confirmation_data(job_id)
    
    if format == 'excel':
        # Create DataFrame
        df = pd.DataFrame(results)
        
        # Export to Excel with formatting
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Enriched Leads', index=False)
            
            # Get worksheet for formatting
            worksheet = writer.sheets['Enriched Leads']
            
            # Apply dropdown validation to confirmed_ethnicity column (Q)
            create_ethnicity_dropdown_validation(
                worksheet, 
                start_row=2, 
                end_row=len(df) + 1, 
                column_letter='Q'
            )
            
            # Apply confidence-based color coding
            apply_confidence_color_coding(worksheet, 2, len(df) + 1)
            
            # Auto-adjust column widths
            for column in worksheet.columns:
                max_length = max(len(str(cell.value)) for cell in column)
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
    
    elif format == 'csv':
        df = pd.DataFrame(results)
        df.to_csv(output_path, index=False)
    
    else:
        raise ValueError(f"Unsupported format: {format}")
    
    return output_path
```

## ðŸ”„ **Confirmation Upload System**

### **CLI Commands for Confirmation Management**

```bash
# Export job with confirmation columns
poetry run leadscout jobs export-for-confirmation <job-id> --output leads_for_dialler.xlsx

# Upload confirmed ethnicities from Excel
poetry run leadscout ethnicity upload-confirmations --file confirmed_leads.xlsx --job-id <job-id>

# Single ethnicity confirmation
poetry run leadscout ethnicity confirm --job-id <job-id> --row-number 25 \
  --ethnicity "African" --confirmed-by "Sales Rep 1" \
  --notes "Confirmed during qualification call"

# Bulk confirmation from CSV
poetry run leadscout ethnicity bulk-confirm --csv confirmations.csv

# View confirmation status for job
poetry run leadscout ethnicity status <job-id>

# Spatial analysis with confirmations
poetry run leadscout ethnicity analyze-spatial --suburb "Sandton" --include-confirmations
```

### **Confirmation Upload Validation**

```python
class EthnicityConfirmationUploader:
    """Handle confirmation uploads with strict validation."""
    
    def __init__(self):
        self.validator = EthnicityValidator()
        self.confirmation_db = EthnicityConfirmationDatabase()
    
    async def upload_confirmations_from_excel(self, file_path, job_id):
        """Upload confirmations from Excel file with validation."""
        
        df = pd.read_excel(file_path)
        
        # Validate required columns
        required_columns = [
            'source_row_number', 'director_ethnicity', 
            'confirmed_ethnicity', 'DirectorName'
        ]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        confirmations = []
        errors = []
        
        for index, row in df.iterrows():
            try:
                # Skip empty confirmations
                if pd.isna(row['confirmed_ethnicity']) or row['confirmed_ethnicity'] == '':
                    continue
                
                # Validate ethnicity
                validated_ethnicity, error = self.validator.validate_ethnicity(
                    row['confirmed_ethnicity']
                )
                
                if error:
                    errors.append(f"Row {index + 2}: {error}")
                    continue
                
                # Create confirmation
                confirmation = {
                    'source_row_number': int(row['source_row_number']),
                    'confirmed_ethnicity': validated_ethnicity,
                    'confirmed_by': row.get('confirmed_by', 'Unknown'),
                    'confirmation_notes': row.get('confirmation_notes', ''),
                    'confirmed_at': datetime.now(),
                    'job_id': job_id
                }
                
                confirmations.append(confirmation)
                
            except Exception as e:
                errors.append(f"Row {index + 2}: {str(e)}")
        
        # Upload valid confirmations
        if confirmations:
            await self.confirmation_db.bulk_update_confirmations(confirmations)
        
        # Return results
        return {
            'uploaded_count': len(confirmations),
            'error_count': len(errors),
            'errors': errors
        }
```

## ðŸ“Š **Learning Integration & Spatial Intelligence**

### **Enhanced Spatial Pattern Learning**

```python
async def update_spatial_patterns_from_confirmations():
    """Update spatial patterns based on human confirmations."""
    
    # Get recent confirmations
    recent_confirmations = await get_recent_confirmations(days=7)
    
    for confirmation in recent_confirmations:
        # Extract name components
        name_components = extract_name_components(confirmation.original_director_name)
        
        for component in name_components:
            # Update or create spatial pattern
            pattern = {
                'name_component': component,
                'suburb': confirmation.canonical_suburb,
                'city': confirmation.canonical_city,
                'province': confirmation.canonical_province,
                'ethnicity_code': confirmation.confirmed_ethnicity,
                'confidence_score': 1.0,  # Human confirmed = 100% confidence
            }
            
            await upsert_spatial_ethnicity_pattern(pattern)
    
    # Recalculate pattern effectiveness
    await recalculate_pattern_success_rates()

async def enhanced_ethnicity_prediction_with_confirmations(name, city, province, suburb=None):
    """Enhanced prediction using confirmed spatial patterns."""
    
    # Look for exact spatial matches first (highest confidence)
    spatial_matches = await lookup_confirmed_spatial_patterns(name, suburb, city, province)
    
    if spatial_matches:
        return create_prediction_from_spatial_match(spatial_matches[0])
    
    # Fall back to existing classification pipeline
    return await classify_ethnicity_standard_pipeline(name, city, province)
```

## ðŸŽ¯ **Success Criteria & Implementation Plan**

### **Phase 1: Core Confirmation System (1-2 weeks)**
- [ ] Enhanced database schema with source tracking
- [ ] File identifier generation and tracking
- [ ] Basic export with confirmation columns
- [ ] Excel dropdown validation implementation

### **Phase 2: Confirmation Upload (1-2 weeks)**
- [ ] CLI commands for confirmation management
- [ ] Excel upload validation and processing
- [ ] Bulk confirmation handling
- [ ] Error reporting and validation

### **Phase 3: Learning Integration (1-2 weeks)**
- [ ] Spatial pattern learning from confirmations
- [ ] Enhanced ethnicity prediction with confirmed data
- [ ] Pattern effectiveness tracking
- [ ] Analytics and reporting for confirmation impact

### **Success Metrics:**
- âœ… **100% record traceability** - Every ethnicity links to exact source row
- âœ… **95%+ confirmation upload success rate** - Reliable validation and processing
- âœ… **20%+ prediction improvement** - Confirmed data enhances AI accuracy
- âœ… **Zero manual lookup time** - Dialler team has all context in Excel
- âœ… **Seamless workflow integration** - Fits existing job processing pipeline

### **Business Value Validation:**
- âœ… **Dialler team efficiency** - Clean, enriched leads with AI predictions
- âœ… **Data quality improvement** - Confirmed ethnicities improve system accuracy
- âœ… **Spatial intelligence** - Name+place correlations enable better targeting
- âœ… **Learning system enhancement** - Human feedback improves AI over time
- âœ… **Audit trail completeness** - Full lifecycle tracking for compliance

---

**Implementation Priority**: Critical - Core requirement for dialler team workflow  
**Dependencies**: Existing ethnicity classification system, job processing pipeline  
**Integration Points**: Export system, CLI commands, learning database  
**Success Pattern**: Builds on proven name classification + adds human feedback loop